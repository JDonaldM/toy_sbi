{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Posterior Estimation\n",
    "\n",
    "In this notebook we train a Mixture Density Network (MDN) for Neural Posterior Estimation (NPE).\n",
    "\n",
    "In this example we will:\n",
    "1. Generate some simulated data.\n",
    "2. Train a MDN.\n",
    "3. Make predictions for the model parameters posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tfm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating some data\n",
    "\n",
    "Like many all of the other notebooks in this repo we start by simulating our data. For the full version of this see `MCMC_zeus.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(100) # We set the random seed for numpy so we get the\n",
    "                    # same synthetic data each time the notebook is run.\n",
    "\n",
    "def straight_line(x, m, c):\n",
    "    ''' A straight line model: y = m*x + c '''\n",
    "    return m*x + c\n",
    "\n",
    "m_true = 3.5 \n",
    "c_true = 1.2 \n",
    "\n",
    "num_points = 70\n",
    "x = np.linspace(0, 10, num_points)\n",
    "\n",
    "sigma = 3.0\n",
    "\n",
    "data = np.random.normal(loc=straight_line(x, m_true, c_true), scale=sigma)\n",
    "\n",
    "fig, ax = plt.subplots(2,1,sharex=True, figsize=(7,5), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "ax[0].errorbar(x, data, yerr=sigma, fmt=\"o\", color='k', zorder=1, label='Data')\n",
    "ax[0].plot(x, straight_line(x, m_true, c_true), '-', lw=2, label='Model')\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel(r'$f(x)$')\n",
    "\n",
    "ax[1].errorbar(x, (data-straight_line(x, m_true, c_true))/sigma, yerr=1., fmt=\"o\")\n",
    "ax[1].axhline(0., linestyle='--', color='grey')\n",
    "ax[1].axhspan(-1,1, alpha=0.1, color='grey')\n",
    "ax[1].axhspan(-2,2, alpha=0.1, color='grey')\n",
    "ax[1].set_ylim(-3,3)\n",
    "ax[1].set_xlabel(r'$x$')\n",
    "ax[1].set_ylabel(r'$\\Delta f \\ / \\ \\sigma$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a MDN.\n",
    "\n",
    "We start by defining the functions required to generate the necessary training data.\n",
    "\n",
    "First we define a function that allows us to randomly sample from the prior. As with all other notebooks in this repo our prior takes the form of a uniform prior on the intercept $c \\sim \\mathcal{U}(-10,10)$ and a normal prior on the gradient $m \\sim \\mathcal{N}(3, 10)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(N):\n",
    "    \n",
    "    rand_c = np.random.uniform(-10., 10., size=(N, ))\n",
    "    rand_m = np.random.normal(loc=3., scale=10., size=(N, ))\n",
    "    \n",
    "    return np.vstack([rand_m, rand_c]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a 'simulator' function. This function generates simulated observations by making model predictions and introduces noise at the same level that we have in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(theta, sigma, x):\n",
    "    \n",
    "    models = theta[:,0]*x.reshape(-1,1) + theta[:,1]\n",
    "\n",
    "    return np.random.normal(loc=models, scale=sigma).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these functions to generate some training data. We generate 50,000 examples, this is similar to the total number of likelihood evaluations when running our MCMC, see `MCMC_zeus.ipynb`. We use 40,000 for training and 10,000 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 2), (40000, 70))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX = prior(40000)\n",
    "trainY = simulator(trainX, sigma, x)\n",
    "\n",
    "valX = prior(10000)\n",
    "valY = simulator(valX, sigma, x)\n",
    "\n",
    "trainX.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our MDN we standardize our training and validations data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yscaler = StandardScaler()\n",
    "xscaler = StandardScaler()\n",
    "\n",
    "trainy = yscaler.fit_transform(trainY)\n",
    "valy = yscaler.transform(valY)\n",
    "\n",
    "trainx = xscaler.fit_transform(trainX)\n",
    "valx = xscaler.transform(valX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our MDN. A MDN predicts parameters of a series of component distributions, e.g. a series of Gaussians. The distribution predicted by the network can be written as\n",
    "\n",
    "$$P(x|y, w) = \\sum_i^{n_c} r_i(y, w)\\mathcal{N}\\left[\\mu_i(y,w), C_i(y,w)\\right],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $w$ are the weights of the MDN, and $n_c$ is the number of component Gaussians.\n",
    "\n",
    "For this simple example we only need to use a single component. It is very simple to define this network using [tensorflow-probability](https://www.tensorflow.org/probability/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer1 = Input(shape=(int(trainy.shape[1]),))\n",
    "\n",
    "d1 = Dense(25, activation = 'tanh')(input_layer1)\n",
    "d2 = Dense(25, activation = 'tanh')(d1)\n",
    "d3 = Dense(25, activation = 'tanh')(d2)\n",
    "d4 = Dense(25, activation = 'tanh')(d3)\n",
    "d5 = Dense(25, activation = 'tanh')(d4)\n",
    "output = Dense(5, activation = 'linear')(d5)\n",
    "\n",
    "dist1 = tfp.layers.MultivariateNormalTriL(2, name=\"dist1\")(output)\n",
    "\n",
    "MDN = Model(inputs=input_layer1, outputs=dist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is the negative log-probability $-\\ln{P(x|y, w)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neglogprob = lambda y, p_y: -p_y.log_prob(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our MDN. We define a callback that will terminate training when the loss function plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamie/opt/anaconda3/envs/tf_prob/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 1.7356 - val_loss: 1.0662\n",
      "Epoch 2/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6788 - val_loss: 0.2478\n",
      "Epoch 3/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -0.0345 - val_loss: -0.2581\n",
      "Epoch 4/200\n",
      "2500/2500 [==============================] - 2s 992us/step - loss: -0.3806 - val_loss: -0.5137\n",
      "Epoch 5/200\n",
      "2500/2500 [==============================] - 2s 989us/step - loss: -0.6007 - val_loss: -0.7095\n",
      "Epoch 6/200\n",
      "2500/2500 [==============================] - 2s 983us/step - loss: -0.7815 - val_loss: -0.8756\n",
      "Epoch 7/200\n",
      "2500/2500 [==============================] - 2s 992us/step - loss: -0.9391 - val_loss: -1.0265\n",
      "Epoch 8/200\n",
      "2500/2500 [==============================] - 2s 995us/step - loss: -1.0802 - val_loss: -1.1691\n",
      "Epoch 9/200\n",
      "2500/2500 [==============================] - 2s 993us/step - loss: -1.2079 - val_loss: -1.2754\n",
      "Epoch 10/200\n",
      "2500/2500 [==============================] - 2s 998us/step - loss: -1.3211 - val_loss: -1.3999\n",
      "Epoch 11/200\n",
      "2500/2500 [==============================] - 2s 991us/step - loss: -1.4228 - val_loss: -1.4949\n",
      "Epoch 12/200\n",
      "2500/2500 [==============================] - 2s 995us/step - loss: -1.5154 - val_loss: -1.5862\n",
      "Epoch 13/200\n",
      "2500/2500 [==============================] - 2s 987us/step - loss: -1.5991 - val_loss: -1.6670\n",
      "Epoch 14/200\n",
      "2500/2500 [==============================] - 2s 998us/step - loss: -1.6764 - val_loss: -1.7402\n",
      "Epoch 15/200\n",
      "2500/2500 [==============================] - 2s 993us/step - loss: -1.7483 - val_loss: -1.8004\n",
      "Epoch 16/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -1.8151 - val_loss: -1.8693\n",
      "Epoch 17/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -1.8750 - val_loss: -1.9288\n",
      "Epoch 18/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -1.9325 - val_loss: -1.9737\n",
      "Epoch 19/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -1.9867 - val_loss: -2.0339\n",
      "Epoch 20/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.0362 - val_loss: -2.0760\n",
      "Epoch 21/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.0828 - val_loss: -2.1240\n",
      "Epoch 22/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.1281 - val_loss: -2.1657\n",
      "Epoch 23/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.1691 - val_loss: -2.2039\n",
      "Epoch 24/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.2092 - val_loss: -2.2282\n",
      "Epoch 25/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.2466 - val_loss: -2.2738\n",
      "Epoch 26/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.2824 - val_loss: -2.2952\n",
      "Epoch 27/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.3164 - val_loss: -2.3415\n",
      "Epoch 28/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.3488 - val_loss: -2.3702\n",
      "Epoch 29/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.3797 - val_loss: -2.3984\n",
      "Epoch 30/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.4112 - val_loss: -2.4309\n",
      "Epoch 31/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.4402 - val_loss: -2.4620\n",
      "Epoch 32/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.4680 - val_loss: -2.4880\n",
      "Epoch 33/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.4974 - val_loss: -2.5164\n",
      "Epoch 34/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.5225 - val_loss: -2.5385\n",
      "Epoch 35/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.5496 - val_loss: -2.5596\n",
      "Epoch 36/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.5753 - val_loss: -2.5858\n",
      "Epoch 37/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.5992 - val_loss: -2.5937\n",
      "Epoch 38/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.6231 - val_loss: -2.6285\n",
      "Epoch 39/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.6466 - val_loss: -2.6620\n",
      "Epoch 40/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.6690 - val_loss: -2.6725\n",
      "Epoch 41/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.6915 - val_loss: -2.7023\n",
      "Epoch 42/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.7134 - val_loss: -2.7145\n",
      "Epoch 43/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.7355 - val_loss: -2.7371\n",
      "Epoch 44/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.7553 - val_loss: -2.7610\n",
      "Epoch 45/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.7770 - val_loss: -2.7907\n",
      "Epoch 46/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.7968 - val_loss: -2.7876\n",
      "Epoch 47/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.8169 - val_loss: -2.8260\n",
      "Epoch 48/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.8374 - val_loss: -2.8499\n",
      "Epoch 49/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.8558 - val_loss: -2.8530\n",
      "Epoch 50/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.8756 - val_loss: -2.8812\n",
      "Epoch 51/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.8920 - val_loss: -2.8964\n",
      "Epoch 52/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.9109 - val_loss: -2.9221\n",
      "Epoch 53/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.9286 - val_loss: -2.9345\n",
      "Epoch 54/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.9459 - val_loss: -2.9471\n",
      "Epoch 55/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.9615 - val_loss: -2.9544\n",
      "Epoch 56/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.9785 - val_loss: -2.9837\n",
      "Epoch 57/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -2.9946 - val_loss: -3.0008\n",
      "Epoch 58/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.0105 - val_loss: -3.0104\n",
      "Epoch 59/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.0276 - val_loss: -3.0093\n",
      "Epoch 60/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.0413 - val_loss: -3.0475\n",
      "Epoch 61/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.0579 - val_loss: -3.0566\n",
      "Epoch 62/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.0725 - val_loss: -3.0701\n",
      "Epoch 63/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.0861 - val_loss: -3.0852\n",
      "Epoch 64/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1009 - val_loss: -3.0985\n",
      "Epoch 65/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1150 - val_loss: -3.1220\n",
      "Epoch 66/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1295 - val_loss: -3.1197\n",
      "Epoch 67/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1425 - val_loss: -3.1445\n",
      "Epoch 68/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1547 - val_loss: -3.1437\n",
      "Epoch 69/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1701 - val_loss: -3.1556\n",
      "Epoch 70/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1803 - val_loss: -3.1836\n",
      "Epoch 71/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.1939 - val_loss: -3.1982\n",
      "Epoch 72/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2053 - val_loss: -3.2075\n",
      "Epoch 73/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2167 - val_loss: -3.2038\n",
      "Epoch 74/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2292 - val_loss: -3.1941\n",
      "Epoch 75/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2417 - val_loss: -3.2235\n",
      "Epoch 76/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2501 - val_loss: -3.2494\n",
      "Epoch 77/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2625 - val_loss: -3.2626\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2741 - val_loss: -3.2698\n",
      "Epoch 79/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2834 - val_loss: -3.2809\n",
      "Epoch 80/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.2931 - val_loss: -3.2888\n",
      "Epoch 81/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3050 - val_loss: -3.3032\n",
      "Epoch 82/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3137 - val_loss: -3.3120\n",
      "Epoch 83/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3223 - val_loss: -3.3165\n",
      "Epoch 84/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3322 - val_loss: -3.3292\n",
      "Epoch 85/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3427 - val_loss: -3.3353\n",
      "Epoch 86/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3527 - val_loss: -3.3414\n",
      "Epoch 87/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3594 - val_loss: -3.3524\n",
      "Epoch 88/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3688 - val_loss: -3.3637\n",
      "Epoch 89/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3766 - val_loss: -3.3634\n",
      "Epoch 90/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3839 - val_loss: -3.3481\n",
      "Epoch 91/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.3930 - val_loss: -3.3845\n",
      "Epoch 92/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4034 - val_loss: -3.3819\n",
      "Epoch 93/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4104 - val_loss: -3.3901\n",
      "Epoch 94/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4176 - val_loss: -3.4104\n",
      "Epoch 95/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4256 - val_loss: -3.4275\n",
      "Epoch 96/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4330 - val_loss: -3.4310\n",
      "Epoch 97/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4390 - val_loss: -3.3775\n",
      "Epoch 98/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4462 - val_loss: -3.4387\n",
      "Epoch 99/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4521 - val_loss: -3.4455\n",
      "Epoch 100/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4608 - val_loss: -3.4568\n",
      "Epoch 101/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4685 - val_loss: -3.4630\n",
      "Epoch 102/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4746 - val_loss: -3.4478\n",
      "Epoch 103/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4800 - val_loss: -3.4622\n",
      "Epoch 104/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4861 - val_loss: -3.4850\n",
      "Epoch 105/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.4936 - val_loss: -3.4773\n",
      "Epoch 106/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5018 - val_loss: -3.4815\n",
      "Epoch 107/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5058 - val_loss: -3.4992\n",
      "Epoch 108/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5110 - val_loss: -3.4964\n",
      "Epoch 109/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5195 - val_loss: -3.5102\n",
      "Epoch 110/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5261 - val_loss: -3.5095\n",
      "Epoch 111/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5287 - val_loss: -3.4945\n",
      "Epoch 112/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5337 - val_loss: -3.5006\n",
      "Epoch 113/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5408 - val_loss: -3.5322\n",
      "Epoch 114/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5453 - val_loss: -3.5366\n",
      "Epoch 115/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5521 - val_loss: -3.5113\n",
      "Epoch 116/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5562 - val_loss: -3.5337\n",
      "Epoch 117/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5621 - val_loss: -3.5351\n",
      "Epoch 118/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5672 - val_loss: -3.5636\n",
      "Epoch 119/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5719 - val_loss: -3.5681\n",
      "Epoch 120/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5766 - val_loss: -3.5666\n",
      "Epoch 121/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5811 - val_loss: -3.5606\n",
      "Epoch 122/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5874 - val_loss: -3.5674\n",
      "Epoch 123/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5911 - val_loss: -3.5830\n",
      "Epoch 124/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.5978 - val_loss: -3.5726\n",
      "Epoch 125/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6011 - val_loss: -3.5989\n",
      "Epoch 126/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6076 - val_loss: -3.6022\n",
      "Epoch 127/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6109 - val_loss: -3.6098\n",
      "Epoch 128/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6146 - val_loss: -3.5918\n",
      "Epoch 129/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6188 - val_loss: -3.6158\n",
      "Epoch 130/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6244 - val_loss: -3.6123\n",
      "Epoch 131/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6284 - val_loss: -3.6196\n",
      "Epoch 132/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6319 - val_loss: -3.6324\n",
      "Epoch 133/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6366 - val_loss: -3.6340\n",
      "Epoch 134/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6398 - val_loss: -3.6332\n",
      "Epoch 135/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6453 - val_loss: -3.5952\n",
      "Epoch 136/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6502 - val_loss: -3.6303\n",
      "Epoch 137/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6545 - val_loss: -3.6354\n",
      "Epoch 138/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6564 - val_loss: -3.6478\n",
      "Epoch 139/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6610 - val_loss: -3.6279\n",
      "Epoch 140/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6647 - val_loss: -3.6426\n",
      "Epoch 141/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6677 - val_loss: -3.6300\n",
      "Epoch 142/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6704 - val_loss: -3.6603\n",
      "Epoch 143/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6773 - val_loss: -3.6548\n",
      "Epoch 144/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6795 - val_loss: -3.6574\n",
      "Epoch 145/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6845 - val_loss: -3.6714\n",
      "Epoch 146/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6870 - val_loss: -3.6638\n",
      "Epoch 147/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6907 - val_loss: -3.6845\n",
      "Epoch 148/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6958 - val_loss: -3.6566\n",
      "Epoch 149/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.6959 - val_loss: -3.6982\n",
      "Epoch 150/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.7030 - val_loss: -3.6875\n",
      "Epoch 151/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.7035 - val_loss: -3.6682\n",
      "Epoch 152/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.7088 - val_loss: -3.6924\n",
      "Epoch 153/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.7118 - val_loss: -3.6979\n",
      "Epoch 154/200\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: -3.7149 - val_loss: -3.6883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7c4a67850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                           verbose=0, mode='min', baseline=None,\n",
    "                           restore_best_weights=True)\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "MDN.compile(optimizer=Adam(lr=0.00001), loss=neglogprob)\n",
    "MDN.fit(trainy,trainx, validation_data=[valy, valx] , epochs=200, batch_size=16, verbose=1,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions is very simple. We however need to make sure we standardize our data in the same way as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = MDN(yscaler.transform(data.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate samples from the prdicted distribution and reverse the standardization so that our samples have physical meaning. We save the samples so that they can be compared to the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = xscaler.inverse_transform(np.array(posterior.sample(5000)[:,0,:]))\n",
    "np.save(\"NPE_samples\", samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_prob",
   "language": "python",
   "name": "tf_prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
